# choose the API to use
# - pollination is free, thanks to them https://pollinations.ai/
# - ollama is a free local server, you can use a tiny model like gemma3n:e2b
__TAIRMINAL_ENGINE=pollinations

# if Using ollama, default to local with very tiny model
__TAIRMINAL_OLLAMA_API=http://localhost:11434/api/generate
__TAIRMINAL_OLLAMA_MODEL="gemma3n:e2b"

# if Using pollinations, use the text-to-text API
__TAIRMINAL_POLLINATIONS_API=https://text.pollinations.ai

# System prompt to avoid any interpretation of the command
# we only want the raw command to be returned
__TAIRMINAL_SYSTEM="You must answer only with the raw Bash command that solves 
the user's request. Do not include any explanations, comments, or formatting 
(no backticks, no Markdown, no code block). Respond with the command only, as 
if to paste it directly in the terminal. No line breaks, no quotes, no prefix, 
no trailing punctuation."

# display information about the command
__TAIRMINAL_SHOW_INFO=true

# support for both yq and jq
__TAIRMINAL_JQ=$(command -v yq || command -v jq)
#__TAIRMINAL_JQ=jq

# get the right jq/yq options for the streaming output
if [ $__TAIRMINAL_JQ == "jq" ]; then
  __TAIRMINAL_JQ_STREAM_OPTS="-c -R -r -j"
  __TAIRMINAL_JQ_STREAM_QUERY='fromjson?|.choices[0].delta.content // empty'
  __TAIRMINAL_JQ_OLLAMA_QUERY='fromjson?|.response'
else
  __TAIRMINAL_JQ_STREAM_OPTS="-p json -0 -r -op"
  __TAIRMINAL_JQ_STREAM_QUERY="--expression '.choices?[0].delta.content // \"\"'"
  __TAIRMINAL_JQ_OLLAMA_QUERY='.response'
fi

# The shortcut to generate a command
__TAIRMINAL_SHORTCUT="\C-x\C-o"

__load_tairminal() {
  for config_file in ~/.local/share/tairminal/config ~/.config/tairminal/config; do
    if [[ -f "$config_file" ]]; then
      eval "$(
        source "$config_file" 2>/dev/null
        [[ -n "$ENGINE" ]] && printf '__TAIRMINAL_ENGINE=%q\n' "$ENGINE"
        [[ -n "$OLLAMA_API" ]] && printf '__TAIRMINAL_OLLAMA_API=%q\n' "$OLLAMA_API"
        [[ -n "$OLLAMA_MODEL" ]] && printf '__TAIRMINAL_OLLAMA_MODEL=%q\n' "$OLLAMA_MODEL"
        [[ -n "$SYSTEM" ]] && printf '__TAIRMINAL_SYSTEM=%q\n' "$SYSTEM"
        [[ -n "$SHOW_INFO" ]] && printf '__TAIRMINAL_SHOW_INFO=%q\n' "$SHOW_INFO"
        [[ -n "$SHORTCUT" ]] && printf '__TAIRMINAL_SHORTCUT=%q\n' "$SHORTCUT"
      )"
      break
    fi
  done
}

__tairminal_ollama() {
  local PROMPT=$1
  local SYSTEM=$2
  local stream=${3:-"false"}

  local MODEL=$__TAIRMINAL_OLLAMA_MODEL
  AI_CMD=$(
    cat <<'EOF'
  http --ignore-stdin --timeout 60 --follow $([ "$stream" == "true" ] && echo "--stream") \
      POST $__TAIRMINAL_OLLAMA_API \
      model="$MODEL" \
      prompt="$PROMPT" \
      system="$SYSTEM" \
      stream:=${stream}
EOF
  )
  if [ "$stream" == "true" ]; then
    eval "$AI_CMD | ${__TAIRMINAL_JQ} ${__TAIRMINAL_JQ_STREAM_OPTS} ${__TAIRMINAL_JQ_OLLAMA_QUERY} 2>/dev/null"
  else
    local RESPONSE=$(eval "$AI_CMD")
    local CMD=$(echo "$RESPONSE" | ${__TAIRMINAL_JQ} ${__TAIRMINAL_JQ_STREAM_OPTS} "${__TAIRMINAL_JQ_OLLAMA_QUERY}" 2>/dev/null)
    [ -z "$CMD" ] && CMD=$"# ❌ No response"
    echo "$CMD"
  fi
}
__tairminal_pollinations() {
  local PROMPT=$1
  local SYSTEM=$2
  local stream=${3:-"false"}

  AI_CMD=$(
    cat <<'EOF'
    http --ignore-stdin --timeout 60 --follow $([ "$stream" == "true" ] && echo "--stream") \
      ${__TAIRMINAL_POLLINATIONS_API}/"$PROMPT" \
      private==true \
      $([ "$stream" == "true" ] && echo "stream==true") \
      system=="$SYSTEM"
EOF
  )
  if [ "$stream" == "true" ]; then
    eval "$AI_CMD | sed 's/^data: //' | ${__TAIRMINAL_JQ} ${__TAIRMINAL_JQ_STREAM_OPTS} ${__TAIRMINAL_JQ_STREAM_QUERY} 2>/dev/null"
    echo
  else
    local RESPONSE=$(eval "$AI_CMD")
    local CMD=$(echo "$RESPONSE")
    echo "$CMD"
  fi

}

__tairminal_generate() {
  __load_tairminal

  # The current line in the readline buffer
  local INPUT="$READLINE_LINE"

  # check if the line starts with a comment character
  if [[ "$INPUT" != \#* ]]; then
    echo "❌ Line should be a comment (statring by #)"
    return
  fi

  # drop the comment character
  local PROMPT="${INPUT#\# }"

  case $__TAIRMINAL_ENGINE in
  ollama)
    CMD=$(__tairminal_ollama "$PROMPT" "$__TAIRMINAL_SYSTEM")
    $__TAIRMINAL_SHOW_INFO && CMD=$(echo -ne $"# Generated by tairminal using $__TAIRMINAL_ENGINE model $__TAIRMINAL_OLLAMA_MODEL\n$CMD")
    ;;
  pollinations)
    CMD=$(__tairminal_pollinations "$PROMPT" "$__TAIRMINAL_SYSTEM")
    $__TAIRMINAL_SHOW_INFO && CMD=$(echo -ne $"# Generated by tairminal using Pollinations API\n$CMD")
    ;;
  *)
    CMD=$"# ❌ unsupported api: $__TAIRMINAL_ENGINE"
    ;;
  esac

  [ -z "$CMD" ] && CMD=$"# ❌ No response"
  READLINE_LINE="$CMD"
  READLINE_POINT=${#CMD}
}

# call AI with default system
__tairminal_ask() {
  local PROMPT=$1
  case $__TAIRMINAL_ENGINE in
  ollama)
    __tairminal_ollama "$PROMPT" "" "true"
    ;;
  pollinations)
    __tairminal_pollinations "$PROMPT" "" "true"
    ;;
  *)
    echo $" ❌ Unsupported engine:" $__TAIRMINAL_ENGINE
    ;;
  esac
}

# a finction to ask whatever the question to the AI
ai() {
  __load_tairminal
  __tairminal_ask "$@"
}

# bind the shortcut for the command generator
__bind_command=$(printf '"%s": __tairminal_generate' "$__TAIRMINAL_SHORTCUT")
bind -x "$__bind_command"
