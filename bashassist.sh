# choose the API to use
# - pollination is free, thanks to them https://pollinations.ai/
# - ollama is a free local server, you can use a tiny model like gemma3n:e2b
__B_ASSIST_ENGINE=pollinations

# if Using ollama, default to local with very tiny model
__B_ASSIST_OLLAMA_API=http://localhost:11434/api/generate
__B_ASSIST_OLLAMA_MODEL="gemma3n:e2b"

# if Using pollinations, use the text-to-text API
__B_ASSIST_POLLINATIONS_API=https://text.pollinations.ai

# System prompt to avoid any interpretation of the command
# we only want the raw command to be returned
__B_ASSIST_SYSTEM="You must answer only with the raw Bash command that solves 
the user's request. Do not include any explanations, comments, or formatting 
(no backticks, no Markdown, no code block). Respond with the command only, as 
if to paste it directly in the terminal. No line breaks, no quotes, no prefix, 
no trailing punctuation."

# display information about the command
__B_ASSIST_SHOW_INFO=false

# support for both yq and jq
__B_ASSIST_JQ=$(command -v yq || command -v jq)
#__B_ASSIST_JQ=jq

# get the right jq/yq options for the streaming output
if [ $__B_ASSIST_JQ == "jq" ]; then
  __B_ASSIST_JQ_STREAM_OPTS="-c -R -r -j"
  __B_ASSIST_JQ_STREAM_QUERY='fromjson?|.choices[0].delta.content // empty'
  __B_ASSIST_JQ_OLLAMA_QUERY='fromjson?|.response'
else
  __B_ASSIST_JQ_STREAM_OPTS="-p json -0 -r -op"
  __B_ASSIST_JQ_STREAM_QUERY="--expression '.choices?[0].delta.content // \"\"'"
  __B_ASSIST_JQ_OLLAMA_QUERY='.response'
fi

# The shortcut to generate a command
__B_ASSIST_SHORTCUT="\C-x\C-o"

__load___b_assist() {
  for config_file in ~/.local/share/bashassist/config ~/.config/bashassist/config; do
    if [[ -f "$config_file" ]]; then
      eval "$(
        source "$config_file" 2>/dev/null
        [[ -n "$ENGINE" ]] && printf '__B_ASSIST_ENGINE=%q\n' "$ENGINE"
        [[ -n "$OLLAMA_API" ]] && printf '__B_ASSIST_OLLAMA_API=%q\n' "$OLLAMA_API"
        [[ -n "$OLLAMA_MODEL" ]] && printf '__B_ASSIST_OLLAMA_MODEL=%q\n' "$OLLAMA_MODEL"
        [[ -n "$SYSTEM" ]] && printf '__B_ASSIST_SYSTEM=%q\n' "$SYSTEM"
        [[ -n "$SHOW_INFO" ]] && printf '__B_ASSIST_SHOW_INFO=%q\n' "$SHOW_INFO"
        [[ -n "$SHORTCUT" ]] && printf '__B_ASSIST_SHORTCUT=%q\n' "$SHORTCUT"
      )"
      break
    fi
  done
}

__b_assist_ollama() {
  local PROMPT=$1
  local SYSTEM=$2
  local stream=${3:-"false"}

  local MODEL=$__B_ASSIST_OLLAMA_MODEL
  AI_CMD=$(
    cat <<'EOF'
  http --ignore-stdin --timeout 60 --follow $([ "$stream" == "true" ] && echo "--stream") \
      POST $__B_ASSIST_OLLAMA_API \
      model="$MODEL" \
      prompt="$PROMPT" \
      system="$SYSTEM" \
      stream:=${stream}
EOF
  )
  if [ "$stream" == "true" ]; then
    eval "$AI_CMD | ${__B_ASSIST_JQ} ${__B_ASSIST_JQ_STREAM_OPTS} ${__B_ASSIST_JQ_OLLAMA_QUERY} 2>/dev/null"
  else
    local RESPONSE=$(eval "$AI_CMD")
    local CMD=$(echo "$RESPONSE" | ${__B_ASSIST_JQ} ${__B_ASSIST_JQ_STREAM_OPTS} "${__B_ASSIST_JQ_OLLAMA_QUERY}" 2>/dev/null)
    [ -z "$CMD" ] && CMD=$"# ❌ No response"
    echo "$CMD"
  fi
}
__b_assist_pollinations() {
  local PROMPT=$1
  local SYSTEM=$2
  local stream=${3:-"false"}

  AI_CMD=$(
    cat <<'EOF'
    http --ignore-stdin --timeout 60 --follow $([ "$stream" == "true" ] && echo "--stream") \
      ${__B_ASSIST_POLLINATIONS_API}/"$PROMPT" \
      private==true \
      $([ "$stream" == "true" ] && echo "stream==true") \
      system=="$SYSTEM"
EOF
  )
  if [ "$stream" == "true" ]; then
    eval "$AI_CMD | sed 's/^data: //' | ${__B_ASSIST_JQ} ${__B_ASSIST_JQ_STREAM_OPTS} ${__B_ASSIST_JQ_STREAM_QUERY} 2>/dev/null"
    echo
  else
    local RESPONSE=$(eval "$AI_CMD")
    local CMD=$(echo "$RESPONSE")
    echo "$CMD"
  fi

}

__b_assist_generate() {
  __load___b_assist

  # The current line in the readline buffer
  local INPUT="$READLINE_LINE"

  # check if the line starts with a comment character
  if [[ "$INPUT" != \#* ]]; then
    echo "❌ Line should be a comment (statring by #)"
    return
  fi

  # drop the comment character
  local PROMPT="${INPUT#\# }"

  case $__B_ASSIST_ENGINE in
  ollama)
    CMD=$(__b_assist_ollama "$PROMPT" "$__B_ASSIST_SYSTEM")
    $__B_ASSIST_SHOW_INFO && CMD=$(echo -ne $"# Generated by bashAssist using $__B_ASSIST_ENGINE model $__B_ASSIST_OLLAMA_MODEL\n$CMD")
    ;;
  pollinations)
    CMD=$(__b_assist_pollinations "$PROMPT" "$__B_ASSIST_SYSTEM")
    $__B_ASSIST_SHOW_INFO && CMD=$(echo -ne $"# Generated by bashAssist using Pollinations API\n$CMD")
    ;;
  *)
    CMD=$"# ❌ unsupported api: $__B_ASSIST_ENGINE"
    ;;
  esac

  [ -z "$CMD" ] && CMD=$"# ❌ No response"
  READLINE_LINE="$CMD"
  READLINE_POINT=${#CMD}
}

# call AI with default system
__b_assist_ask() {
  local PROMPT=$1
  case $__B_ASSIST_ENGINE in
  ollama)
    __b_assist_ollama "$PROMPT" "" "true"
    ;;
  pollinations)
    __b_assist_pollinations "$PROMPT" "" "true"
    ;;
  *)
    echo $" ❌ Unsupported engine:" $__B_ASSIST_ENGINE
    ;;
  esac
}

# a finction to ask whatever the question to the AI
ai() {
  __load___b_assist
  __b_assist_ask "$@"
}

# bind the shortcut for the command generator
__bind_b_assist_command=$(printf '"%s": __b_assist_generate' "$__B_ASSIST_SHORTCUT")
bind -x "$__bind_b_assist_command"
